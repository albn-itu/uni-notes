---
tags:
  - meta
  - exam
---

# Exercises
- [x] Week 2
    - [x] Practial introduction
    - [x] Poses
- [x] Week 4
    - [x] Pen and paper
- [x] Week 5
    - [x] Tutorial
    - [x] Regression (Evaluation part)
- [x] Week 6
    - [x] Missing data
- [x] Week 7
    - [x] Filter basics
- [x] Week 8
    - [x] Decision
    - [x] Logistic regression
    - [x] HOG
- [x] Week 9
    - [x] Metrics
    - [x] HOG
- [x] Week 10
    - [x] PCA
    - [x] Shape generation
- [x] Week 11
    - [x] Clustering
    - [x] Pen and paper
    - [x] Non-linear
- [x] Week 12
    - [x] Neural networks
    - [x] Tuning
- [x] Week 13
    - [x] Colorization

# Exam reading
- [x] Linear Algebra
  - [x] Vectors
    - [x] Inner products
    - [x] Operations
    - [x] Distance metrics
    - [x] Relation to ML
  - [x] Matrices
    - [x] Matrix addition
    - [x] Matrix multiplication
    - [x] Matrix inverse
    - [x] Matrix transpose
    - [x] Matrix determinant
    - [x] Matrix orthogonal
    - [x] Matrix transformations
    - [x] Matrix basis
    - [x] Relate to application in course
    - [x] Their relation to linear equations and their solutions
  - [x] Linear transformations
    - [x] 2D
    - [x] 3D
    - [x] Higher dimensions
    - [x] Non-linear transformations
    - [x] Affine transformations
    - [x] Homogenous coordinates
    - [x] Composition of linear transformations
  - [x] Least Squares
    - [x] Linear least squares
    - [x] Projections
    - [x] Solving linear least square problems for model fitting
    - [x] Mandatory 1
- [x] Data
  - [x] Data cleaning
  - [x] Uncertainty / Descriptive statistics
    - [x] Relate to model learning and data evaluation
  - [x] Data, Uncertainty, over/underfitting
  - [x] Uncertainty and data in relation to regression
  - [x] Uncertainty and data in relation to classification
  - [x] Uncertainty and data in relation to clustering
  - [x] Uncertainty and data in relation to dimensionality reduction
  - [x] Missing data
  - [x] Duplicate data
  - [x] Outlier detection
  - [x] Data imputation
- [x] Regularization
  - [x] Filtering
    - [x] Convolution and Correlation
    - [x] Noise
    - [x] Image gradients
  - [x] Matching and metrics
  - [x] Cross validation
  - [x] Regularization
  - [x] Bias/Variance, R-squared
- [x] Classification
  - [x] Linear classification and kernels
  - [x] Logistic regression
  - [x] Features/HOG features and classification
- [x] Evaluation
  - [x] Metrics/Evaluation of Classifiers
  - [x] Metrics/Evaluation of regression models
  - [x] Features/HOG features and classification 
  - [x] Imbalanced data for classification and regression
- [ ] PCA
  - [ ] Dimensionality reduction
    - [ ] Focus on mandatory 2
  - [ ] Generative models
  - [ ] Eigenvalues
  - [ ] Covariance matrix
  - [ ] Basis
- [ ] Clustering and Non-linear optimization
  - [ ] K-Means and Mean shift
  - [ ] K-Means and Algomerative clustering
  - [ ] K-Means and ELBOW
  - [ ] Non-linear optimization 
    - [ ] Non-linear functions
    - [ ] Graphs
    - [ ] Gradients
    - [ ] Gradient descent with relation to model training and non-linear models
- [ ] Neural Networks
  - [ ] Prediction (regression vs classification)
  - [ ] Training
    - [ ] Gradients
    - [ ] Chain rule
    - [ ] back/foward propagation
  - [ ] Training and evalution
- [ ] Architectures
  - [ ] Model Architectures
    - [ ] MLP
    - [ ] CNN
    - [ ] Difference between fully connected, multi-layer perceptron (MLP) and convolutional neural networks (CNN)
  - [ ] Regularization
  - [ ] Data augmentation
  - [ ] Model complexity
  - [ ] Norms
  - [ ] Model tuning
  - [ ] Dropout
  - [ ] Early stopping
  - [ ] Complexity


# Additional preparation
  - [ ] Linear Algebra
    - [ ] Basics
      - [ ] List as many purposes for which we use vectors for image analysis and Machine learning
      - [ ] What is the equation of a line, planes and hyperplane using vector nota-tion?
      - [ ] How do you calculate the length and orientation of a vector ?
      - [ ] How do we know when two vectors u and v are orthogonal to each other.
      - [ ] How do we know when two vectors u and v are parallel to each other?
    - [ ] Linear Equation
      - [ ] What is a linear equation and how is this relate to matrices?
    - [ ] Inner product
      - [ ] How is the inner product related to:
        - [ ] a measure of distance.
        - [ ] matrix multiplication
        - [ ] projections
        - [ ] convolution
        - [ ] neural networks
    - [ ] Solutions to Linear Equation
      - [ ] What does it mean to have a solution to a linear set of equations?
      - [ ] When can we have one, zero or many solutions to a linear set of equations?
      - [ ] What is an over-determined set of equations.
      - [ ] What is an under-determined set of equations.
      - [ ] Why is the Determinant relevant when talking about solutions to linear equations.
      - [ ] Why are subspaces important when talking about solutions to linear set of equations.
      - [ ] Given data X âˆˆ RN and labels y. How w can linear equations be used to find the coefficients of the following models and how much training data is needed to learn the model parameters
        - [ ] (a) A straight line in the plane
        - [ ] (b) A plane in 3D
        - [ ] (c) A hyperplane in N-dimensional spaces
        - [ ] (d) Find the coefficients of a an N-order polynomial
        - [ ] (e) Find the coefficients of an similarity or affine transformations
      - [ ] In the above cases what is the minimal number of points needed to solve the linear set of equations.
    - [ ] Transformation
      - [ ] What is a transformation and how is it related to a projection.
      - [ ] Matrix multiplications may be considered as a transformation. Why?
      - [ ] How are linear transformations combined?
      - [ ] What is the purpose of homogeneous coordinates.
      - [ ] What is the inverse of a transformation and what is its relevance to the course / ML.
      - [ ] How is least squares (formally) related to projections
  - [ ] Signals
    - [ ] What are the definitions of convolution and correlation and how are they related
    - [ ] When can correlation and convolution be used interchangeably
    - [ ] How can correlation be implemented in a neural architechture and why is this beneficial?
    - [ ] How is image templates useful as a machine learning model and a metric for comparison.
    - [ ] How do image templates relate to machine learning
  - [ ] Machine Learning
    - [ ] Where is supervised and unsupervised learning used and how do they differ.
    - [ ] How is least squares used in machine learning and how does it relate to least squares when using matrices.
    - [ ] What is an objective function
    - [ ] Which methods can be used to learn linear and non-linear models
    - [ ] What is regularization and why is it needed.
    - [ ] How do recommender systems work and how is this related to inner prod-ucts and matrix factorization.
    - [ ] What is PCA? and how does PCA make use of subspaces , eigenvalues and eigenvectors.
    - [ ] How is PCA and certain neural architectures related.
    - [ ] Why is it called linear classification
    - [ ] How does logistic regression differ from linear classifiation.
    - [ ] What is the decision boundary and how can you find it.
    - [ ] what is a kernel, where are they used and how is it related to model learning
  - [ ] Evaluation
    - [ ] Why is evaluation needed
    - [ ] How do you ensure proper evaluation of models
    - [ ] Why are training, test and verification sets needed in the training proce- dures?
    - [ ] What is cross validation and how is it related to overfitting / underfitting
    - [ ] How how can we tell when a model is under and overfitted
